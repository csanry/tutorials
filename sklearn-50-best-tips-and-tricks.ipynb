{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to this Kernel\n",
    "\n",
    "## ***This kernel is a compilation of tricks of sklearn published weekly by Kevin Markham.***\n",
    "\n",
    "You can find the original tricks and tips on the GitHub repo:\n",
    "\n",
    "https://github.com/justmarkham/scikit-learn-tips\n",
    "\n",
    "\n",
    "## ***This kernel is under construction. I will be updating it regularly.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"table_of_contents\"></a>\n",
    "# Table of contents\n",
    "\n",
    "[Importing libraries and setting some helper functions](#Imports)\n",
    "\n",
    "[Trick 50: General pattern to many ML problems](#trick50)\n",
    "\n",
    "[Trick 49: Tune multiple models simultaneously with GridSearchCV](#trick49)\n",
    "\n",
    "[Trick 48: You can access a part of a pipeline using Python slicing](#trick48)\n",
    "\n",
    "[Trick 46: Ensemble multiple methods using VotingClassifier or VotingRegressor](#trick46)\n",
    "\n",
    "[Trick 45: Create polynomial features](#trick45)\n",
    "\n",
    "[Trick 44: Speed up the GridSearchCV](#trick44)\n",
    "\n",
    "[Trick 43: OrdinalEncoder vs OneHotEncoder for tree based models](#trick43)\n",
    "\n",
    "[Trick 42: ColumnTransformer use cases of 'passthrough' and 'drop'](#trick42)\n",
    "\n",
    "[Trick 41: OneHotEncoder, drop colums if it's binary (new in 0.23 and above)](#trick41)\n",
    "\n",
    "[Trick 40: Estimators only print the parameters that are *not* set to their default values (new in 0.23 and above)](#trick40)\n",
    "\n",
    "[Trick 39: Load a toy dataset into a DataFrame (new in 0.23 and above)](#trick39)\n",
    "\n",
    "[Trick 38: Get the features names of a ColumnTransformer (new in 0.23 and above)](#trick38)\n",
    "\n",
    "[Trick 37: Generating interactive pipelines (new in 0.23 and above)](#trick37)\n",
    "\n",
    "[Trick 36: Passing parameters as keyword arguments (new in 0.23 and above)](#trick36)\n",
    "\n",
    "[Trick 35: Passing a df directly to sklearn](#trick35)\n",
    "\n",
    "[Trick 34: Feature selection with Pipeline](#trick34)\n",
    "\n",
    "[Trick 33: Using custom and existing function in a ColumnTransformer](#trick33)\n",
    "\n",
    "[Trick 32: Area Under Curve (AUC) for binary classification: ovo and ovr strategies](#trick32)\n",
    "\n",
    "[Trick 31: Shuffle when using cross_val_score](#trick31)\n",
    "\n",
    "[Trick 30: Four ways of displaying the model coefficients](#trick30)\n",
    "\n",
    "[Trick 29: Vectorize two text columns using ColumnTransformer](#trick29)\n",
    "\n",
    "[Trick 28: Save a model of pipeline using joblib](#trick28)\n",
    "\n",
    "[Trick 27: Imputing missing values for categorical values](#trick27)\n",
    "\n",
    "[Trick 26: Use of stratify when performing classification problems](#trick26)\n",
    "\n",
    "[Trick 25: Prunning decision trees (new in 0.22 and above).](#trick25)\n",
    "\n",
    "[Trick 24: Plotting the decision tree with sklearn (new in 0.21 and above)](#trick24)\n",
    "\n",
    "[Trick 23: Display the intercept & coefficients for a liner model](#trick23)\n",
    "\n",
    "[Trick 22: Two types of Pipelines](#trick22)\n",
    "\n",
    "[Trick 21: Several ROC curves in a single plot (new in sklearn 0.22)](#trick21)\n",
    "\n",
    "[Trick 20: Plot confusion matrix (new in sklearn 0.22)](#trick20)\n",
    "\n",
    "[Trick 19: Most important parameters of a LogisticRegression](#trick19)\n",
    "\n",
    "[Trick 18: Convert your GridSearchCV or RandomizedGridSearch results into a pandas DataFrame](#trick18)\n",
    "\n",
    "[Trick 17: RandomizedGridSearch](#trick17)\n",
    "\n",
    "[Trick 16: Crossvalidate and gridsearch a sklearn pipeline](#trick16)\n",
    "\n",
    "[Trick 15: OneHotEncoder: tips using it](#trick15)\n",
    "\n",
    "[Trick 14: Handling missing values](#trick14)\n",
    "\n",
    "[Trick 13: Examine each step of a Pipeline](#trick13)\n",
    "\n",
    "[Trick 12: Difference between Pipeline and make_pipeline](#trick12)\n",
    "\n",
    "[Trick 11: KNNImputer](#trick11)\n",
    "\n",
    "[Trick 10: Using random_state to reproduce results](#trick10)\n",
    "\n",
    "[Trick 9: Using missing values as a feature: SimpleImputer & add_indicator = True](#trick9)\n",
    "\n",
    "[Trick 8: Using make_pipeline in a ML project](#trick8)\n",
    "\n",
    "[Trick 7: Handle new data while using OneHotEncoder](#trick7)\n",
    "\n",
    "[Trick 6: Common ways to encode categorical features: OneHotEncoder, OrdinalEncoder](#trick6)\n",
    "\n",
    "[Trick 5: Benefits of using sklearn for preprocessing and not pandas](#trick5)\n",
    "\n",
    "[Trick 4: When to use fit_transform and transform methods](#trick4)\n",
    "\n",
    "[Trick 3: Difference between fit and transform method](#trick3)\n",
    "\n",
    "[Trick 2: Seven ways to select columns using ColumnsTransformer](#trick2)\n",
    "\n",
    "[Trick 1: Using column selector to transform different columns](#trick1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"Imports\"></a>\n",
    "# Importing libraries and setting some helper functions\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import Image\n",
    "\n",
    "# this will allow us to print all the files as we generate more in the kernel\n",
    "def print_files(directory = \"output\"):\n",
    "    if directory.lower() == \"input\":\n",
    "        for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "            for filename in filenames:\n",
    "                print(os.path.join(dirname, filename))\n",
    "    else:\n",
    "        for dirname, _, filenames in os.walk('/kaggle/working'):\n",
    "            for filename in filenames:\n",
    "                print(os.path.join(dirname, filename))\n",
    "\n",
    "\n",
    "def generate_sample_data(): # creates a fake df for testing\n",
    "    number_or_rows = 20\n",
    "    num_cols = 7\n",
    "    cols = list(\"ABCDEFG\")\n",
    "    df = pd.DataFrame(np.random.randint(1, 20, size = (number_or_rows, num_cols)), columns=cols)\n",
    "    df.index = pd.util.testing.makeIntIndex(number_or_rows)\n",
    "    return df\n",
    "\n",
    "\n",
    "def generate_sample_data_datetime(): # creates a fake df for testing\n",
    "    number_or_rows = 365*24\n",
    "    num_cols = 2\n",
    "    cols = [\"sales\", \"customers\"]\n",
    "    df = pd.DataFrame(np.random.randint(1, 20, size = (number_or_rows, num_cols)), columns=cols)\n",
    "    df.index = pd.util.testing.makeDateIndex(number_or_rows, freq=\"H\")\n",
    "    return df\n",
    "\n",
    "# show several prints in one cell. This will allow us to condence every trick in one cell.\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "print_files(\"input\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick50\"></a>\n",
    "# Trick 50: General pattern to solve many ML problems\n",
    "[Go back to the Table of Contents](#table_of_contents)\n",
    "\n",
    "Let's build in this cell a general pattern to solve many ML problems\n",
    "\n",
    "Some of the shortcomings it has are:\n",
    "\n",
    "* Assumes all columns have proper data types\n",
    "\n",
    "* May include irrelevant or improper features\n",
    "\n",
    "* Does not handle text or date columns well\n",
    "\n",
    "* Does not include feature engineering\n",
    "\n",
    "* Ordinal encoding may be better\n",
    "\n",
    "* Other imputation strategies may be better\n",
    "\n",
    "* Numeric features may not need scaling\n",
    "\n",
    "* A different model may be better\n",
    "\n",
    "* And so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# import libraries\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import make_column_selector, make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# get the data\n",
    "lc = ['Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n",
    "df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n",
    "X = df[lc]\n",
    "y = df['Survived']\n",
    "\n",
    "X_test = pd.read_csv(\"/kaggle/input/titanic/train.csv\", usecols = lc)\n",
    "\n",
    "# first let's define the steps that will allow us to filter our columns\n",
    "numeric_select = make_column_selector(dtype_include = \"number\")\n",
    "no_numeric_select = make_column_selector(dtype_exclude = 'number')\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (make_pipeline(SimpleImputer(strategy = \"mean\"), StandardScaler()), numeric_select), # select all numeric columns, impute mean and scale the data\n",
    "    (make_pipeline(SimpleImputer(strategy = \"constant\"), OneHotEncoder(handle_unknown='ignore')), no_numeric_select) # select all non numeric columns, impute most frequent value and OneHotEncode\n",
    ") \n",
    "\n",
    "pipe = make_pipeline(preprocessor, LogisticRegression())\n",
    "\n",
    "# see the crossvalidation score\n",
    "print(\"CV score is {}\".format(cross_val_score(pipe, X, y).mean()))\n",
    "\n",
    "# apply the same pipeline and predict\n",
    "# fit the pipeline and make predictions\n",
    "pipe.fit(X, y)\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "print(\"Here we have our predictions\",y_pred[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick49\"></a>\n",
    "# Trick 49: Tune multiple models simultaneously with GridSearchCV\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# import libraries\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# get the data\n",
    "df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n",
    "X = df[['Sex', 'Name', 'Age']]\n",
    "y = df['Survived']\n",
    "\n",
    "# this will be the first Pipeline step\n",
    "ct = ColumnTransformer(\n",
    "    [('ohe', OneHotEncoder(), ['Sex']),\n",
    "     ('vectorizer', CountVectorizer(), 'Name'),\n",
    "     ('imputer', SimpleImputer(), ['Age'])])\n",
    "\n",
    "# each of these models will take a turn as the second Pipeline step\n",
    "clf1 = LogisticRegression(solver='liblinear', random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1)\n",
    "\n",
    "# create the Pipeline\n",
    "pipe = Pipeline([('preprocessor', ct), ('classifier', clf1)])\n",
    "\n",
    "# create the parameter dictionary for clf1\n",
    "params1 = {}\n",
    "params1['preprocessor__vectorizer__ngram_range'] = [(1, 1), (1, 2)]\n",
    "params1['classifier__penalty'] = ['l1', 'l2']\n",
    "params1['classifier__C'] = [0.1, 1, 10]\n",
    "params1['classifier'] = [clf1]\n",
    "\n",
    "# create the parameter dictionary for clf2\n",
    "params2 = {}\n",
    "params2['preprocessor__vectorizer__ngram_range'] = [(1, 1), (1, 2)]\n",
    "params2['classifier__n_estimators'] = [100, 200]\n",
    "params2['classifier__min_samples_leaf'] = [1, 2]\n",
    "params2['classifier'] = [clf2]\n",
    "\n",
    "# create a list of parameter dictionaries\n",
    "params = [params1, params2]\n",
    "\n",
    "# this will search every parameter combination within each dictionary\n",
    "grid = GridSearchCV(pipe, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the best parameters for each method\n",
    "grid.fit(X, y)\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick48\"></a>\n",
    "# Trick 48: You can access a part of a pipeline using Python slicing\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# import libraries\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# get the data\n",
    "df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n",
    "X = df[['Parch', 'Fare', 'Embarked', 'Sex', 'Name', 'Age']]\n",
    "y = df['Survived']\n",
    "\n",
    "imp_constant = SimpleImputer(strategy='constant')\n",
    "ohe = OneHotEncoder()\n",
    "\n",
    "imp_ohe = make_pipeline(imp_constant, ohe)\n",
    "vect = CountVectorizer()\n",
    "imp = SimpleImputer()\n",
    "\n",
    "# pipeline step 1\n",
    "ct = make_column_transformer(\n",
    "    (imp_ohe, ['Embarked', 'Sex']),\n",
    "    (vect, 'Name'),\n",
    "    (imp, ['Age', 'Fare']),\n",
    "    ('passthrough', ['Parch']))\n",
    "\n",
    "# pipeline step 2\n",
    "selection = SelectPercentile(chi2, percentile=50)\n",
    "\n",
    "# pipeline step 3\n",
    "logreg = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# display estimators as diagrams\n",
    "from sklearn import set_config\n",
    "set_config('diagram')\n",
    "\n",
    "pipe = Pipeline([('preprocessor', ct), ('feature selector', selection), ('classifier', logreg)])\n",
    "\n",
    "# now we can use our pipeline to fit_transform everything\n",
    "X_all = pipe.fit(X, y)\n",
    "pipe.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but can also access just a part of the pipeline\n",
    "pipe[1]\n",
    "X_parcial = pipe[0].fit_transform(X)\n",
    "X_parcial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick47\"></a>\n",
    "# Trick 47: Tune the parameters of a VotingClassifier\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# import libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# get the data\n",
    "df = pd.read_csv(\"/kaggle/input/titanic/train.csv\", usecols = ['Survived', 'Pclass', 'Parch', 'SibSp', 'Fare'])\n",
    "df.dropna(inplace = True)\n",
    "# separate X and y\n",
    "X = df.drop(\"Survived\", axis = \"columns\")\n",
    "y = df[\"Survived\"]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# instanciate individual models\n",
    "lr = LogisticRegression()\n",
    "rf = RandomForestClassifier()\n",
    "mnb = MultinomialNB()\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# create an ensemble for improved accuracy\n",
    "vc = VotingClassifier([('clf1', lr), ('clf2', rf), (\"clf3\", mnb), (\"clf4\", dt)], voting = 'soft')\n",
    "print(\"CV Score of a VotingClassifier is {}\".format(cross_val_score(vc, X, y).mean()))\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# GridSearch the best parameters\n",
    "params = {'voting':['hard', 'soft'],\n",
    "          'weights':[(1,1,1,1), (2,1,1,1), (1,2,1,1), (1,1,2,1), (1,1,1,2)]}\n",
    "\n",
    "# find the best set of parameters\n",
    "grid = GridSearchCV(vc, params)\n",
    "grid.fit(X, y)\n",
    "grid.best_params_\n",
    "\n",
    "# accuracy has improved\n",
    "print(\"CV Score of a VotingClassifier with GridSearchCV is {}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick46\"></a>\n",
    "# Trick 46: Ensemble multiple methods using VotingClassifier or VotingRegressor\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# import libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# get the data\n",
    "df = pd.read_csv(\"/kaggle/input/titanic/train.csv\", usecols = ['Survived', 'Pclass', 'Parch', 'SibSp', 'Fare'])\n",
    "df.dropna(inplace = True)\n",
    "# separate X and y\n",
    "X = df.drop(\"Survived\", axis = \"columns\")\n",
    "y = df[\"Survived\"]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# let's fit the individual models\n",
    "lr = LogisticRegression()\n",
    "print(\"CV Score of LogisticRegression is {}\".format(cross_val_score(lr, X, y).mean()))\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "print(\"CV Score of RandomForest is {}\".format(cross_val_score(rf, X, y).mean()))\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# create an ensemble for improved accuracy\n",
    "vc = VotingClassifier([('clf1', lr), ('clf2', rf)], voting = 'soft')\n",
    "print(\"CV Score of a VotingClassifier is {}\".format(cross_val_score(vc, X, y).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick45\"></a>\n",
    "# Trick 45: Create polynomial features\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a very fast way to create features in sklearn\n",
    "# but be careful, it might be time consuming and impracticale for some algorithms\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# import libraries\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(include_bias=False, interaction_only=True)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# get the data\n",
    "X = pd.DataFrame({'A':[1, 2, 3], 'B':[4, 4, 4], 'C':[0, 10, 100]})\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# create new features\n",
    "# Output columns: A, B, C, A*B, A*C, B*C\n",
    "poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick44\"></a>\n",
    "# Trick 44: Speed up the GridSearchCV\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# import libraries\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# get the data\n",
    "df = pd.read_csv(\"/kaggle/input/titanic/train.csv\", usecols = [\"Survived\",\"Sex\",\"Name\", \"Age\"])\n",
    "df.dropna(inplace = True)\n",
    "# separate X and y\n",
    "X = df.drop(\"Survived\", axis = \"columns\")\n",
    "y = df[\"Survived\"]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# make a ColumTransformer, instaciate a model and build a pipeline\n",
    "\n",
    "ct = ColumnTransformer([(\"ohe\", OneHotEncoder(), [\"Sex\"]),\n",
    "                       (\"vectorizer\", CountVectorizer(), \"Name\"),\n",
    "                       (\"imputer\", SimpleImputer(), [\"Age\"])])\n",
    "\n",
    "clf = LogisticRegression(solver='liblinear', random_state=1)\n",
    "\n",
    "pipe = Pipeline([(\"preprocessor\", ct), (\"classifier\", clf)])\n",
    "\n",
    "# set the parameters for the GridSearch\n",
    "\n",
    "params = {}\n",
    "params['preprocessor__ohe__drop'] = [None, 'first']\n",
    "params['preprocessor__vectorizer__min_df'] = [1, 2, 3]\n",
    "params['preprocessor__vectorizer__ngram_range'] = [(1, 1), (1, 2)]\n",
    "params['classifier__C'] = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "params['classifier__penalty'] = ['l2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's time the GridSearch without all CPU\n",
    "\n",
    "grid = GridSearchCV(pipe, params)\n",
    "%time grid.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's time the GridSearch with all CPU\n",
    "# as you can see, when using -1, it's much faster\n",
    "grid = GridSearchCV(pipe, params, n_jobs = -1)\n",
    "%time grid.fit(X, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick43\"></a>\n",
    "# Trick 43: OrdinalEncoder vs OneHotEncoder for tree based models\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# import libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# get the data\n",
    "df = pd.read_csv(\"/kaggle/input/titanic/train.csv\", usecols = [\"Survived\", \"Pclass\", \"Sex\", \"Embarked\"])\n",
    "df.dropna(inplace = True)\n",
    "# separate X and y\n",
    "X = df.drop(\"Survived\", axis = \"columns\")\n",
    "y = df[[\"Survived\"]]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# instanciate 2 encoders\n",
    "oe = OrdinalEncoder()\n",
    "ohe = OneHotEncoder()\n",
    "\n",
    "X_oe = oe.fit_transform(df)\n",
    "X_ohe = ohe.fit_transform(df)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# let's see the shape of a resulting DataFrame using OE or OHE\n",
    "print('The dataframe transformed with OrdinalEncoder has a shape of {}'.format(X_oe.shape))\n",
    "print('The dataframe transformed with OneHotEncoder has a shape of {}'.format(X_ohe.shape))\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# let's measure the time needed for training and the corresponing cv score\n",
    "oe_pipe = make_pipeline(oe, RandomForestClassifier(random_state = 175))\n",
    "ohe_pipe = make_pipeline(ohe, RandomForestClassifier(random_state = 175))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time cross_val_score(oe_pipe, X, y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as you can see, oe is slightly faster and accuracy doesn't suffer that much.\n",
    "%time cross_val_score(ohe_pipe, X, y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick42\"></a>\n",
    "# Trick 42: ColumnTransformer use cases of 'passthrough' and 'drop'\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# import libraries\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# load some data\n",
    "X = pd.DataFrame({'A':[1, 2, np.nan],\n",
    "                  'B':[10, 20, 30],\n",
    "                  'C':[100, 200, 300],\n",
    "                  'D':[1000, 2000, 3000],\n",
    "                  'E':[10000, 20000, 30000]})\n",
    "\n",
    "# use ColumnTransformer to\n",
    "# 1. imputer A\n",
    "# 2. ignore/passthrough B,C\n",
    "# 3. drop D, E\n",
    "\n",
    "ct = make_column_transformer((SimpleImputer(), [\"A\"]),\n",
    "                            (\"passthrough\", [\"B\", \"C\"]),\n",
    "                             remainder = \"drop\")\n",
    "\n",
    "ct.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use ColumnTransformer to\n",
    "# 1. imputer A\n",
    "# 2. drop D, C\n",
    "# 3. ignore/passthrough B, E\n",
    "\n",
    "ct = make_column_transformer((SimpleImputer(), [\"A\"]),\n",
    "                            (\"drop\", [\"D\", \"C\"]),\n",
    "                             remainder = \"passthrough\")\n",
    "\n",
    "ct.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick41\"></a>\n",
    "# Trick 41: OneHotEncoder, drop colums if it's binary (new in 0.23 and above)\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# import libraries\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# load some data\n",
    "df = pd.DataFrame({'Shape':['circle', 'oval', 'square', 'square'],\n",
    "                  'Color': ['pink', 'yellow', 'pink', 'yellow']})\n",
    "\n",
    "# as you can see, color is a binary columns\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the default behaviour creates a colum per category\n",
    "# Note: we change sparse = False, to see our matrix\n",
    "ohe_default = OneHotEncoder(sparse = False).fit_transform(df)\n",
    "ohe_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the default behaviour creates a colum per category\n",
    "# add drop = \"first\" to drop first category in each columns\n",
    "ohe_drop_first = OneHotEncoder(sparse = False, drop = \"first\").fit_transform(df)\n",
    "ohe_drop_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the default behaviour creates a colum per category\n",
    "# new in 0.23 allows you to drop if it's binary\n",
    "# ohe_if_binary = OneHotEncoder(sparse = False, drop = \"if_binary\").fit_transform(df)\n",
    "# ohe_if_binary\n",
    "\n",
    "# in older version you get this error\n",
    "# ValueError: Wrong input for parameter `drop`. Expected 'first', None or array of objects, got <class 'str'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick40\"></a>\n",
    "# Trick 40: Estimators only print the parameters that are *not* set to their default values (new in 0.23 and above)\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# import libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# in old versions of sklearn, when you instanciate an estimator class you will see all it's parameters\n",
    "# in 0.23 and above, you will only see the parameters that have been changed\n",
    "\n",
    "clf_old = LogisticRegression(C = 0.1, solver = \"liblinear\")\n",
    "clf_old\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the new version however, you will only see:\n",
    "# LogisticRegression(C=0.1, solver='liblinear')\n",
    "\n",
    "# to see all parameters\n",
    "clf_old.get_params()\n",
    "\n",
    "# restore old default behaviour\n",
    "# makes sense in 0.23 and above\n",
    "from sklearn import set_config\n",
    "set_config(print_changed_only = False)\n",
    "clf_old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick39\"></a>\n",
    "# Trick 39: Load a toy dataset into a DataFrame (new in 0.23 and above)\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# import libraries\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# load the iris dataset as a DataFrame\n",
    "# this will work in 0.23: as_frame = True\n",
    "# df = load_iris(as_frame = True)[\"frame\"]\n",
    "\n",
    "# return DataFrame with features and Series with target\n",
    "# X, y = load_iris(as_frame=True, return_X_y=True)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# load a dataframe \"old school\" method\n",
    "data = load_iris()\n",
    "\n",
    "X = data[\"data\"]\n",
    "y = data[\"target\"]\n",
    "target_names = data[\"target_names\"] # original values of the target columns\n",
    "columns = data[\"feature_names\"]\n",
    "\n",
    "df = pd.DataFrame(X, columns = columns)\n",
    "df[\"target\"] = y\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick38\"></a>\n",
    "# Trick 38: Get the features names of a ColumnTransformer (new in 0.23 and above)\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# import libraries\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# get the data: select only 4 columns and drop all na\n",
    "X = pd.read_csv(\"/kaggle/input/titanic/train.csv\", usecols = ['Embarked', 'Sex', 'Parch', 'Fare']).dropna()\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# make a column transform\n",
    "ct = make_column_transformer((OneHotEncoder(), [\"Embarked\", \"Sex\"]),\n",
    "                            remainder = \"passthrough\")\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# fit transform our columns transformer and see the resulting shape\n",
    "ct.fit_transform(X).shape\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# get the names of the of the features we have created\n",
    "# in sklearn 0.23 you won't see this error\n",
    "# NotImplementedError: get_feature_names is not yet supported when using a 'passthrough' transformer.\n",
    "# ct.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick37\"></a>\n",
    "# Trick 37: Generating interactive pipelines (new in 0.23 and above)\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When creating large pipelines, create a pipeline to visualize it easier\n",
    "# New in sklearn 0.23\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# import libraries\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# get the data\n",
    "df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n",
    "X = df[['Parch', 'Fare', 'Embarked', 'Sex', 'Name', 'Age']]\n",
    "y = df['Survived']\n",
    "\n",
    "imp_constant = SimpleImputer(strategy='constant')\n",
    "ohe = OneHotEncoder()\n",
    "\n",
    "imp_ohe = make_pipeline(imp_constant, ohe)\n",
    "vect = CountVectorizer()\n",
    "imp = SimpleImputer()\n",
    "\n",
    "# pipeline step 1\n",
    "ct = make_column_transformer(\n",
    "    (imp_ohe, ['Embarked', 'Sex']),\n",
    "    (vect, 'Name'),\n",
    "    (imp, ['Age', 'Fare']),\n",
    "    ('passthrough', ['Parch']))\n",
    "\n",
    "# pipeline step 2\n",
    "selection = SelectPercentile(chi2, percentile=50)\n",
    "\n",
    "# pipeline step 3\n",
    "logreg = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# display estimators as diagrams\n",
    "from sklearn import set_config\n",
    "set_config('diagram')\n",
    "\n",
    "pipe = make_pipeline(ct, selection, logreg)\n",
    "pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick36\"></a>\n",
    "# Trick 36: Passing parameters as keyword arguments (new in 0.23 and above)\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting from sklearn 0.23 we must pass the parameters for the functions and classes as \n",
    "# keyword and not as positional argument. Otherwise a warning will be raised\n",
    "\n",
    "import sklearn\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# here we won't see because, Kaggle uses a previous version\n",
    "print(sklearn.__version__)\n",
    "\n",
    "# positional argument\n",
    "clf = SVC(0.1, 'linear')\n",
    "\n",
    "# keyword arguments\n",
    "clf = SVC(C=0.1, kernel='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick35\"></a>\n",
    "# Trick 35: Passing a df directly to sklearn\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't need to use .values when passing a df or a pandas series to sklearn\n",
    "# It knows internally how to acess the values and deal with them\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# import libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# get the data\n",
    "df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n",
    "X = df[['Pclass', 'Fare']]\n",
    "y = df[\"Survived\"]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# check the X and y types\n",
    "print(type(X))\n",
    "print(type(y))\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# instanciate our classes\n",
    "model = LogisticRegression()\n",
    "\n",
    "# we fit directly a df and a series and sklearn deals with the rest\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick34\"></a>\n",
    "# Trick 34: Feature selection with Pipeline\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# import libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# get the data\n",
    "df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n",
    "X = df[\"Name\"]\n",
    "y = df[\"Survived\"]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# instanciate our classes\n",
    "vectorizer = CountVectorizer()\n",
    "model = LogisticRegression()\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# make the pipeline without feature selection\n",
    "pipe = make_pipeline(vectorizer, model)\n",
    "score = cross_val_score(pipe, X, y, scoring = 'accuracy').mean()\n",
    "print(\"Score of pipeline without feature selection is {}\".format(score))\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# make the pipeline without feature selection\n",
    "\n",
    "# keep 50% of features with the best chi-squared scores\n",
    "selection = SelectPercentile(chi2, percentile = 50)\n",
    "\n",
    "# add the selection after preprocessing but before model\n",
    "pipe = make_pipeline(vectorizer, selection, model)\n",
    "score = cross_val_score(pipe, X, y, scoring = 'accuracy').mean()\n",
    "print(\"Score of pipeline with feature selection is {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick33\"></a>\n",
    "# Trick 33: Using custom and existing function in a ColumnTransformer\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# import libraries\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# get some data\n",
    "X = pd.DataFrame({'Fare':[200, 300, 50, 900],\n",
    "                  'Code':['X12', 'Y20', 'Z7', np.nan],\n",
    "                  'Deck':['A101', 'C102', 'A200', 'C300']})\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# use an existing column and make the function compatible with Pipeline\n",
    "clip_values = FunctionTransformer(np.clip, kw_args={'a_min':100, 'a_max':600})\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# create a custom function\n",
    "def first_letter(string_column):\n",
    "    return string_column.apply(lambda x: x.str.slice(0, 1))\n",
    "\n",
    "# now use FunctionTransformer to make the function compatible with Pipeline\n",
    "get_first_letter = FunctionTransformer(first_letter)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# create the column Transformer\n",
    "ct = make_column_transformer(\n",
    "    (clip_values, ['Fare']),\n",
    "    (get_first_letter, ['Code', 'Deck']))\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Original X\n",
    "print(\"Original X\")\n",
    "X\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Modified X\n",
    "print(\"Modified X\")\n",
    "ct.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick32\"></a>\n",
    "# Trick 32: Area Under Curve (AUC) for binary classification: ovo and ovr strategies\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# import libraries\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# get X and y for classification\n",
    "X, y = load_wine(return_X_y=True)\n",
    "\n",
    "# select only a few features\n",
    "X = X[:, 0:2]\n",
    "\n",
    "# instanciate the model for regression\n",
    "model_clf = LogisticRegression()\n",
    "\n",
    "# Multiclass AUC with train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "model_clf.fit(X_train, y_train)\n",
    "y_score = model_clf.predict_proba(X_test)\n",
    "\n",
    "# use 'ovo' (One-vs-One) or 'ovr' (One-vs-Rest)\n",
    "print(\"roc_auc_score is {} for one vs one strategy with train test split\".format(roc_auc_score(y_test, y_score, multi_class = 'ovo')))\n",
    "print(\"roc_auc_score is {} for one vs rest strategy with train test split\".format(roc_auc_score(y_test, y_score, multi_class = 'ovr')))\n",
    "print(\"-----------------------------------------\")\n",
    "\n",
    "# Multiclass AUC with cross-validation\n",
    "# use 'roc_auc_ovo' (One-vs-One) or 'roc_auc_ovr' (One-vs-Rest)\n",
    "print(\"cross_val_score is {} for one vs one strategy with cross validation\".format(cross_val_score(model_clf, X, y, cv = 5, scoring = 'roc_auc_ovo').mean()))\n",
    "print(\"cross_val_score is {} for one vs rest strategy with cross validation\".format(cross_val_score(model_clf, X, y, cv = 5, scoring = 'roc_auc_ovr').mean()));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick31\"></a>\n",
    "# Trick 31: Shuffle when using cross_val_score\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# import libraries\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "# get X and y for regression\n",
    "X_reg, y_reg = load_diabetes(return_X_y = True)\n",
    "\n",
    "# get X and y for classification\n",
    "df = pd.read_csv(\"/kaggle/input/titanic/train.csv\", usecols = ['Pclass', 'Fare', 'SibSp', 'Survived']).dropna()\n",
    "\n",
    "# separate X and y\n",
    "X_clf = df[['Pclass', 'Fare', 'SibSp']]\n",
    "y_clf = df[[\"Survived\"]]\n",
    "\n",
    "# instanciate the model for regression\n",
    "model_reg = LinearRegression()\n",
    "\n",
    "# instanciate the model for classification\n",
    "model_clf = LogisticRegression()\n",
    "\n",
    "# Use KFold for regression\n",
    "kf = KFold(5, shuffle = True, random_state = 1)\n",
    "print(\"cross_val_score for regression model\")\n",
    "cross_val_score(model_reg, # the regression model, in our case, LinearRegression\n",
    "                X_reg, # X: features to learn from\n",
    "                y_reg, # y: what the predict\n",
    "                cv = kf, # cross_validation scheme we have created earlier\n",
    "                scoring = \"r2\") # metric to use to validate the quality of the model\n",
    "\n",
    "# Use StratifiedKFold for classification\n",
    "skf = StratifiedKFold(5, shuffle = True, random_state = 1)\n",
    "print(\"cross_val_score for classification model\")\n",
    "cross_val_score(model_clf, # the model, in our case, LogisticRegression\n",
    "                X_clf, # X: features to learn from\n",
    "                y_clf, # y: what the predict\n",
    "                cv = skf, # cross_validation scheme we have created earlier\n",
    "                scoring = \"accuracy\") # metric to use to validate the quality of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick30\"></a>\n",
    "# Trick 30: Four ways of displaying the model coefficients\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# import libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# instanciate the classes\n",
    "ohe = OneHotEncoder()\n",
    "model = LogisticRegression()\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# get the data\n",
    "df = pd.read_csv(\"/kaggle/input/titanic/train.csv\", usecols = ['Embarked', 'Survived']).dropna()\n",
    "\n",
    "# separate X and y\n",
    "X = df[[\"Embarked\"]]\n",
    "y = df[[\"Survived\"]]\n",
    "\n",
    "# create a pipeline and fit the X and y\n",
    "pipe = Pipeline([(\"ohe\", ohe),\n",
    "                 (\"clf\", model)])\n",
    "pipe.fit(X, y)\n",
    "\n",
    "# inspect the coefficients\n",
    "print(\"1 way to show model coefficients\")\n",
    "pipe.named_steps.clf.coef_\n",
    "print(\"2 way to show model coefficients\")\n",
    "pipe.named_steps[\"clf\"].coef_\n",
    "print(\"3 way to show model coefficients\")\n",
    "pipe[\"clf\"].coef_\n",
    "print(\"4 way to show model coefficients\")\n",
    "pipe[1].coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick29\"></a>\n",
    "# Trick 29: Vectorize two text columns using ColumnTransformer\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# import libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# get train data\n",
    "df_train = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n",
    "\n",
    "# drop any nans\n",
    "X = df_train[[\"Name\", \"Cabin\"]].dropna()\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# instanciate CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# instanciate CountVectorizer\n",
    "# You can pass the CountVectorizer multiple times and it will learn\n",
    "# separate vocabularies.\n",
    "# to do so, you must use make_column_transformer\n",
    "ct = make_column_transformer((count_vect, 'Name'), (count_vect, 'Cabin'))\n",
    "X_transform = ct.fit_transform(X)\n",
    "X_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick28\"></a>\n",
    "# Trick 28: Save a model of pipeline using joblib\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# import libraries\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import joblib\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# get train data\n",
    "df_train = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n",
    "\n",
    "# drop any nans\n",
    "df_train.dropna(axis = \"rows\", inplace = True)\n",
    "\n",
    "# separate X and y\n",
    "cols_for_x = [\"Embarked\", \"Sex\"]\n",
    "X_train = df_train[cols_for_x]\n",
    "y_train = df_train[\"Survived\"]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# get test data\n",
    "df_test = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n",
    "\n",
    "# drop any nans\n",
    "df_test.dropna(axis = \"rows\", inplace = True)\n",
    "\n",
    "X_test = df_test[cols_for_x]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# instanciate Ohe and make_pipeline\n",
    "ohe = OneHotEncoder()\n",
    "model = LogisticRegression()\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# create the pipeline\n",
    "pipe = make_pipeline(ohe, model)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# predict_using pipeline\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# save pipeline\n",
    "joblib.dump(pipe, 'pipe.joblib')\n",
    "\n",
    "# print our newly saved pipeline\n",
    "print_files()\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# save pipeline\n",
    "new_pipe = joblib.load('/kaggle/working/pipe.joblib')\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# predict using the same pipe and the old pipe\n",
    "print(\"------------\")\n",
    "print(\"Old pipe.\")\n",
    "pipe.predict(X_test)\n",
    "print(\"------------\")\n",
    "print(\"Old pipe.\")\n",
    "new_pipe.predict(X_test)\n",
    "print(\"Notice that both pipes predict the same result.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick27\"></a>\n",
    "# Trick 27: Imputing missing values for categorical values\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# get some fake data\n",
    "d = {\"Shape_Original\":[\"square\", \"square\", \"square\", \"oval\", \"circle\", np.nan]}\n",
    "df = pd.DataFrame(d)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# import libraries\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# impute values using most frequent\n",
    "df[\"most_frequent\"] = SimpleImputer(strategy = \"most_frequent\").fit_transform(df[[\"Shape_Original\"]])\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# impute values using most constant\n",
    "df[\"constant\"]  = SimpleImputer(strategy = \"constant\", fill_value = \"missing\").fit_transform(df[[\"Shape_Original\"]])\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# the result of our imputation\n",
    "df.style.apply(lambda x: ['background: lightgreen' if x.name == 5 else '' for i in x], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick26\"></a>\n",
    "# Trick 26: Use of stratify when performing classification problems\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# import libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# generate some data and separate X and y\n",
    "df = pd.DataFrame({'feature':list(range(8)), 'target':['not fraud']*6 + ['fraud']*2})\n",
    "X = df[['feature']]\n",
    "y = df['target']\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# train and test without stratify\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5, random_state = 1)\n",
    "\n",
    "print(\"y_train withous stratify\")\n",
    "y_train\n",
    "print(\"y_test withous stratify\")\n",
    "y_test\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# train and test with stratify\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5, stratify = y, random_state = 1)\n",
    "\n",
    "print(\"Notice how using statify preserves the fraud and not fraud percentage.\")\n",
    "print(\"y_train with stratify\")\n",
    "y_train\n",
    "print(\"y_test with stratify\")\n",
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick25\"></a>\n",
    "# Trick 25: Prunning decision trees (new in 0.22 and above).\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# import libraries\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# import data\n",
    "df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n",
    "df['Sex'] = df['Sex'].map({'male':0, 'female':1})\n",
    "X = df[[\"Pclass\", \"Fare\", \"Sex\"]]\n",
    "y = df[\"Survived\"]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# basic model and evaluation\n",
    "model = DecisionTreeClassifier(random_state = 175)\n",
    "model.fit(X, y)\n",
    "score = cross_val_score(model, X, y, scoring = \"accuracy\")\n",
    "print(\"Our DecissionTree with {} nodes has scored {}\".format(model.tree_.node_count, score.mean()))\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# prun the tree and see cross validation score\n",
    "# Notice that the score went up. Prunnig trees has a lot of benefits, the main one is reducing overfitting.\n",
    "# ccp_alpha is the parameter that controls the decision tree complexity (cost complexity parameter).\n",
    "# Greater values of ccp_alpha increase the number of nodes pruned.\n",
    "# documentation \n",
    "# https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html\n",
    "\n",
    "model = DecisionTreeClassifier(ccp_alpha = 0.001, random_state = 175)\n",
    "model.fit(X, y)\n",
    "score = cross_val_score(model, X, y, scoring = \"accuracy\")\n",
    "print(\"Our prunned DecissionTree with {} nodes has scored {}\".format(model.tree_.node_count, score.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick24\"></a>\n",
    "# Trick 24: Plotting the decision tree with sklearn (new in 0.21 and above).\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# import libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# create our instances\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# import data\n",
    "df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n",
    "df['Sex'] = df['Sex'].map({'male':0, 'female':1})\n",
    "X = df[[\"Pclass\", \"Fare\", \"Sex\"]]\n",
    "y = df[\"Survived\"]\n",
    "\n",
    "features = [\"Pclass\", \"Fare\", \"Sex\"]\n",
    "classes = [\"Survived\"]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# train test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# fit and predict\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# plot the tree\n",
    "\n",
    "plt.figure(figsize = (20, 10))\n",
    "plot_tree(model, feature_names = features, filled = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# show the text\n",
    "# I will plot only the first 200 characters of the tree since it grows rapidly\n",
    "print(export_text(model, feature_names = features, show_weights=True)[:200]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick23\"></a>\n",
    "# Trick 23: Display the intercept & coefficients for a liner model\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# import libraries\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# load data and separate X and y\n",
    "dataset = load_diabetes()\n",
    "X, y = dataset.data, dataset.target\n",
    "features = dataset.feature_names\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# fit model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# intercept and coef\n",
    "model.intercept_\n",
    "model.coef_\n",
    "list(zip(features, model.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick22\"></a>\n",
    "# Trick 22: Two types of Pipelines\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# Two types of ROC Curve\n",
    "\n",
    "# If the pipeline ends in a classifier or regressor, you use the fit and predict methods\n",
    "# If the pipeline ends in a transformer you use the fit_transform and transform methods\n",
    "\n",
    "path = \"/kaggle/input/roc-curve/ROC Curve.jpeg\"\n",
    "Image(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick21\"></a>\n",
    "# Trick 21: Several ROC curves in a single plot (new in sklearn 0.22 and above)\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# import libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# create our instances\n",
    "lr = LogisticRegression()\n",
    "dt = DecisionTreeClassifier()\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# import data\n",
    "df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n",
    "X = df[[\"Pclass\", \"Fare\"]]\n",
    "y = df[\"Survived\"]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# fit and predict\n",
    "lr.fit(X_train, y_train)\n",
    "dt.fit(X_train, y_train)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# plot roc curve\n",
    "disp = plot_roc_curve(lr, X_test, y_test)\n",
    "plot_roc_curve(dt, X_test, y_test, ax = disp.ax_) # ax = disp.ax_ this line will share the x axis\n",
    "plot_roc_curve(rf, X_test, y_test, ax = disp.ax_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick20\"></a>\n",
    "# Trick 20: Plot confusion matrix (new in sklearn 0.22 and above)\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# import libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# create our instances\n",
    "model = LogisticRegression(random_state = 1)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# import data\n",
    "df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n",
    "X = df[[\"Pclass\", \"Fare\"]]\n",
    "y = df[\"Survived\"]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# train test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# fit and predict\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# plot confusion matrix\n",
    "# notice that you have to pass the model, X_test and y_test\n",
    "# plot_confusion_matrix predicts with the model and plots the values\n",
    "\n",
    "disp = plot_confusion_matrix(model, X_test, y_test, cmap = \"Blues\", values_format = \".3g\")\n",
    "\n",
    "print(\"The classical confusion matrix\")\n",
    "disp.confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick19\"></a>\n",
    "# Trick 19: Most important parameters of a LogisticRegression\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# C: inverse of regularization strength\n",
    "# penalty: type of regularization\n",
    "# solver: algorithm used for optimization\n",
    "\n",
    "path = \"/kaggle/input/logisticregression/LogisticRegression.jpg\"\n",
    "Image(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick18\"></a>\n",
    "# Trick 18: Convert your GridSearchCV or RandomizedGridSearch results into a pandas DataFrame\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# import libraries\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# create our instances\n",
    "ohe = OneHotEncoder()\n",
    "vect = CountVectorizer()\n",
    "ct = make_column_transformer((ohe, [\"Sex\"]), (vect, \"Name\"))\n",
    "model = LogisticRegression(solver = \"liblinear\", random_state = 1)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# import data\n",
    "df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n",
    "X = df[[\"Sex\", \"Name\", \"Fare\"]]\n",
    "y = df[\"Survived\"]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# make pipeline\n",
    "pipeline = make_pipeline(ct, model)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# cross validate the entire pipeline\n",
    "print(\"Notice the score of our entire pipeline is {}\".format(cross_val_score(pipeline, X, y, cv = 5, scoring = \"accuracy\").mean()))\n",
    "cross_val_score(pipeline, X, y, cv = 5, scoring = \"accuracy\").mean()\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# gridsearch the entire pipeline\n",
    "\n",
    "# set the parameters\n",
    "params = {\"columntransformer__countvectorizer__min_df\":[1, 2],\n",
    "         \"logisticregression__C\":[0.1, 1, 10],\n",
    "         \"logisticregression__penalty\":[\"l1\", \"l2\"]}\n",
    "\n",
    "grid = GridSearchCV(pipeline, params, cv = 5, scoring = \"accuracy\")\n",
    "grid.fit(X, y)\n",
    "\n",
    "# convert to a pandas DataFrame\n",
    "\n",
    "results = pd.DataFrame(grid.cv_results_)[[\"params\", \"mean_test_score\", \"rank_test_score\"]]\n",
    "results.sort_values(\"rank_test_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick17\"></a>\n",
    "# Trick 17: RandomizedGridSearch\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# import libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import scipy as sp\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# create our instances\n",
    "vect = CountVectorizer()\n",
    "model = MultinomialNB()\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# make pipeline\n",
    "pipeline = make_pipeline(vect, model)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# import data\n",
    "df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n",
    "X = df['Name']\n",
    "y = df['Survived']\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# set the params to optimize\n",
    "\n",
    "params = {}\n",
    "\n",
    "params[\"countvectorizer__min_df\"] = [1, 2, 3, 4]\n",
    "params[\"countvectorizer__lowercase\"] = [True, False]\n",
    "params[\"multinomialnb__alpha\"] = sp.stats.uniform(scale = 1)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# optimize\n",
    "\n",
    "rand = RandomizedSearchCV(pipeline, params, n_iter = 10, cv = 5, scoring = \"accuracy\", random_state = 1)\n",
    "rand.fit(X, y)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# best score and params\n",
    "\n",
    "print(\"Best score achieved with our search is:\")\n",
    "rand.best_score_\n",
    "\n",
    "print(\"Best params are:\")\n",
    "rand.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick16\"></a>\n",
    "# Trick 16: Crossvalidate and gridsearch a sklearn pipeline\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# import libraries\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# create our instances\n",
    "ohe = OneHotEncoder()\n",
    "vect = CountVectorizer()\n",
    "ct = make_column_transformer((ohe, [\"Sex\"]), (vect, \"Name\"))\n",
    "model = LogisticRegression(solver = \"liblinear\", random_state = 1)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# import data\n",
    "df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n",
    "X = df[[\"Sex\", \"Name\", \"Fare\"]]\n",
    "y = df[\"Survived\"]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# make pipeline\n",
    "pipeline = make_pipeline(ct, model)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# cross validate the entire pipeline\n",
    "print(\"Notice the score of our entire pipeline is {}\".format(cross_val_score(pipeline, X, y, cv = 5, scoring = \"accuracy\").mean()))\n",
    "cross_val_score(pipeline, X, y, cv = 5, scoring = \"accuracy\").mean()\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# gridsearch the entire pipeline\n",
    "\n",
    "# set the parameters\n",
    "params = {\"columntransformer__countvectorizer__min_df\":[1, 2],\n",
    "         \"logisticregression__C\":[0.1, 1, 10],\n",
    "         \"logisticregression__penalty\":[\"l1\", \"l2\"]}\n",
    "\n",
    "grid = GridSearchCV(pipeline, params, cv = 5, scoring = \"accuracy\")\n",
    "grid.fit(X, y)\n",
    "\n",
    "# see the best score\n",
    "print(\"#-------------------------------------------------------------------------\")\n",
    "print(\"Best score of the GridSearchCV is \")\n",
    "grid.best_score_\n",
    "\n",
    "# see the best params\n",
    "print(\"#-------------------------------------------------------------------------\")\n",
    "print(\"Best parameters of the GridSearchCV are \")\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick15\"></a>\n",
    "# Trick 15: OneHotEncoder: tips using it\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use OneHotEncoder, don't \"drop = 'first'\":\n",
    "1. Multicollinearity is rarely and issue with sklearn models\n",
    "2. drop = 'first' is incompatible with handle_unknown = 'ignore'\n",
    "3. May cause you problems if yoy standarize all features or use a regularized model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick14\"></a>\n",
    "# Trick 14: Handling missing values\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# If you have missing values you can:\n",
    "# 1. Drop all rows with missing values\n",
    "# 2. Drop all colmns with missing values\n",
    "# 3. Impute missing values\n",
    "# 4. Use a model that handles missing values\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# import libraries\n",
    "from sklearn.experimental import enable_hist_gradient_boosting # this import enables \"experimental\" packages and clases in sklearn\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier # this is an experimental package\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# import data\n",
    "df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n",
    "print(\"We can see that we have missing values\")\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# split target and feature\n",
    "\n",
    "features = [col for col in df.columns if df[col].dtype != \"object\"] # select only numerical columns\n",
    "features.remove(\"Survived\")\n",
    "features.remove(\"PassengerId\")\n",
    "\n",
    "X = df[features]\n",
    "y = df[\"Survived\"]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Train a model that handles missing values\n",
    "\n",
    "model = HistGradientBoostingClassifier()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 175)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy of our model while having missing values is {}%\".format(round(accuracy_score(y_test, y_pred), 2)*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick13\"></a>\n",
    "# Trick 13: Examine each step of a Pipeline\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# import libraries\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# create each instance\n",
    "si = SimpleImputer()\n",
    "model = LogisticRegression()\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# import data\n",
    "df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# select columns to transform\n",
    "X = df[[\"Fare\", \"Age\"]].head()\n",
    "X[\"Age\"].iloc[0] = np.nan # create a missing values\n",
    "X.head()\n",
    "\n",
    "y = df[[\"Survived\"]].head()\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# use make_pipeline\n",
    "\n",
    "pipeline = make_pipeline(si, model)\n",
    "\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# let's see the statistics of each step\n",
    "print(\"These are the imputed values with the SimpleImputer\")\n",
    "pipeline.named_steps.simpleimputer.statistics_\n",
    "\n",
    "print(\"Display the coefficients of the linear model\")\n",
    "pipeline.named_steps.logisticregression.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick12\"></a>\n",
    "# Trick 12: Difference between Pipeline and make_pipeline\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# import libraries\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import make_column_transformer, ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# create each instance\n",
    "ohe = OneHotEncoder()\n",
    "si = SimpleImputer()\n",
    "model = LogisticRegression()\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# import data\n",
    "df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# select columns to transform\n",
    "X = df[[\"Fare\", \"Embarked\", \"Sex\", \"Age\"]].head()\n",
    "X[\"Age\"].iloc[0] = np.nan # create a missing values\n",
    "X.head()\n",
    "\n",
    "y = df[[\"Survived\"]].head()\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# use make_pipeline\n",
    "\n",
    "column_transformer = make_column_transformer(\n",
    "(ohe, [\"Embarked\", \"Sex\"]),\n",
    "(si, [\"Age\"]),\n",
    "remainder = \"passthrough\"\n",
    ")\n",
    "\n",
    "pipeline = make_pipeline(column_transformer, model)\n",
    "\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# use Pipeline\n",
    "# The main difference is that we must name each step\n",
    "\n",
    "column_transformer = ColumnTransformer(\n",
    "[(\"encoder\", ohe, [\"Embarked\", \"Sex\"]), # notice how we must name each step\n",
    "(\"imputer\", si, [\"Age\"])],\n",
    "remainder = \"passthrough\"\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([(\"preprocessing\", column_transformer), (\"model\", model)]) # notice how we must name each step\n",
    "pipeline.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick11\"></a>\n",
    "# Trick 11: KNNImputer\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# import libraries\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# create some random data\n",
    "df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n",
    "df.isnull().sum() # inspect nulls\n",
    "df.head()\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# let's see KNNInputer in action\n",
    "knn_inputer = KNNImputer()\n",
    "\n",
    "X = df[[\"SibSp\", \"Fare\", \"Age\"]]\n",
    "nan_index = X[X[\"Age\"].isnull()].index\n",
    "\n",
    "print(\"Data with nans\")\n",
    "X[X.index.isin(nan_index)].head(10)\n",
    "\n",
    "print(\"Transformed data with no nans, and the values are based on the NN.\")\n",
    "X_transformed = pd.DataFrame(knn_inputer.fit_transform(X), columns = [\"SibSp\", \"Fare\", \"Age\"], index = X.index)\n",
    "X_transformed[X.index.isin(nan_index)].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick10\"></a>\n",
    "# Trick 10: Using random_state to reproduce results\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# import libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# create some random data\n",
    "\n",
    "df = generate_sample_data()\n",
    "target = [np.random.choice([0, 1]) for i in range(len(df))]\n",
    "df[\"target\"] = target\n",
    "print(\"A sneak peak at our df.\")\n",
    "df.head(3)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# train and target columns\n",
    "features = list(df.columns)[:-1] # all except the last one\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# split nr 1 using random_state to reproduce results\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(df[features], df[\"target\"], test_size = 0.1, random_state = 175)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# split nr 2 using random_state to reproduce results\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(df[features], df[\"target\"], test_size = 0.1, random_state = 175)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# split nr 3 using with no random_state\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(df[features], df[\"target\"], test_size = 0.1)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# let's look at our results\n",
    "print(\"First train test split.\")\n",
    "X_train1.head()\n",
    "print(\"Second train test split. Equal to the first one.\")\n",
    "X_train2.head()\n",
    "X_train1.index == X_train2.index\n",
    "print(\"Third train test split. Different than the others.\")\n",
    "X_train3.head()\n",
    "X_train1.index == X_train3.index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick9\"></a>\n",
    "# Trick 9: Using missing values as a feature: SimpleImputer & add_indicator = True\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# import the libraries\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# create some train and test data\n",
    "\n",
    "# create a train df\n",
    "d = {\n",
    "\"Age\":[10, 20, np.nan, 30, 15, 10, 40, 10, np.nan]\n",
    "}\n",
    "\n",
    "print(\"Train data\")\n",
    "df_train = pd.DataFrame(d)\n",
    "df_train\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Sometimes there is a relathionship between missing values and the target\n",
    "# We can use this information creating a new features while performing an imputation of a missing values\n",
    "\n",
    "# normal SimpleImputer()\n",
    "imputer = SimpleImputer()\n",
    "df_transformed = imputer.fit_transform(df_train)\n",
    "df_transformed\n",
    "\n",
    "# SimpleImputer() with the parameter add_indicator\n",
    "print(\"Notice aditional column with an indicator of 1 next to the previously missing values\")\n",
    "imputer = SimpleImputer(add_indicator = True)\n",
    "df_transformed = imputer.fit_transform(df_train)\n",
    "df_transformed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick8\"></a>\n",
    "# Trick 8: Using make_pipeline in a ML project\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# import the libraries\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# create some train and test data\n",
    "\n",
    "# create a train df\n",
    "d = {\n",
    "\"feat1\":[10, 20, np.nan, 2],\n",
    "\"feat2\":[25, 20, 5, 3],\n",
    "\"target\":[\"A\", \"A\", \"B\", \"B\"]\n",
    "}\n",
    "\n",
    "print(\"Train data\")\n",
    "df_train = pd.DataFrame(d)\n",
    "df_train\n",
    "\n",
    "# create a test df\n",
    "d = {\n",
    "\"feat1\":[30, 5, 15],\n",
    "\"feat2\":[12, 10, np.nan]\n",
    "}\n",
    "\n",
    "print(\"Test data\")\n",
    "df_test = pd.DataFrame(d)\n",
    "df_test\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# simple ML project step by step\n",
    "\n",
    "# create each instance\n",
    "si = SimpleImputer()\n",
    "model = LogisticRegression()\n",
    "pipeline = make_pipeline(si, model)\n",
    "\n",
    "# separate the data between target and features\n",
    "features = [\"feat1\", \"feat2\"]\n",
    "X_train, y_train = df_train[features], df_train[\"target\"]\n",
    "X_test = df_test[features]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# use pipeline to fit and predict\n",
    "\n",
    "# First pipeline will use the SimpleImputer to imputer the missing values\n",
    "# Then it will train using LogisticRegression\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# When used pipeline to predict, it will do the same steps as in fit\n",
    "pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick7\"></a>\n",
    "# Trick 7: Handle new data while using OneHotEncoder\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# create a train df\n",
    "d = {\n",
    "\"Categorical\":[\"A\", \"A\", \"B\", \"C\"]\n",
    "}\n",
    "\n",
    "df_train = pd.DataFrame(d)\n",
    "df_train\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# import the libraries\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# transform data during train part\n",
    "print(\"fit_transform using OneHotEncoder.\")\n",
    "ohe = OneHotEncoder(sparse = False, handle_unknown = \"ignore\") # if you don't put false, you will get a sparse matrix object\n",
    "X_train = ohe.fit_transform(df_train[[\"Categorical\"]])\n",
    "X_train\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# create a test df\n",
    "\n",
    "d = {\n",
    "\"Categorical\":[\"A\", \"A\", \"B\", \"C\", \"D\"] # new value, D, previously not seen in train\n",
    "}\n",
    "\n",
    "df_test = pd.DataFrame(d)\n",
    "df_test\n",
    "\n",
    "\n",
    "print(\"transform using OneHotEncoder. Notice that we have a line with zeros for categorical value of D\")\n",
    "X_test = ohe.transform(df_test[[\"Categorical\"]])\n",
    "X_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick6\"></a>\n",
    "# Trick 6: Common ways to encode categorical features: OneHotEncoder, OrdinalEncoder\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# create a df\n",
    "d = {\n",
    "\"Shape\":[\"square\", \"square\", \"oval\", \"circle\"],\n",
    "\"Class\":[\"third\", \"first\", \"second\", \"third\"],\n",
    "\"Size\":[\"S\", \"S\", \"L\", \"XL\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(d)\n",
    "df\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# import the libraries\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# transform data using OneHotEncoder\n",
    "print(\"Transform categorical data using OneHotEncoder\")\n",
    "ohe = OneHotEncoder(sparse = False) # if you don't put false, you will get a sparse matrix object\n",
    "shaped_transformed = ohe.fit_transform(df[[\"Shape\"]]) # if you pass as a series, you will need to reshape the data. Notice the double square bracket\n",
    "shaped_transformed\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# transform data using OrdinalEncoder\n",
    "print(\"Transform categorical data using OrdinalEncoder\")\n",
    "print(\"When using OrdinalEncoder, your data has to have a order: like first class, second class, third class\")\n",
    "oe = OrdinalEncoder(categories = [[\"first\", \"second\", \"third\"], # order for the column Class\n",
    "                                  [\"S\", \"M\", \"L\", \"XL\"]]) # order for the column Size\n",
    "categorical_ordinal_transformed = oe.fit_transform(df[[\"Class\", \"Size\"]])\n",
    "categorical_ordinal_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick5\"></a>\n",
    "# Trick 5: Benefits of using sklearn for preprocessing and not pandas\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reasons to use sklearn to ML preprocessing and not pandas\n",
    "\n",
    "1. You can cross-validate the entire workflow\n",
    "2. You can grid search model & preprocessing hyperparameters\n",
    "3. Avoids adding new columns to the source DataFrame\n",
    "4. pandas lacks separate fit/transform steps to prevent data leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick4\"></a>\n",
    "# Trick 4: When to use fit_transform and transform methods\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Use \"fit_transform\" on training data, but \"transform\" (only) on testing/new data.***\n",
    "\n",
    "Applies the same transformations to both sets of data, which creates consistent columns and prevents data leakage!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick3\"></a>\n",
    "# Trick 3: Difference between fit and transform method\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: What is the difference between the \"fit\" and \"transform\" methods?\n",
    "\n",
    "#### ***\"fit\"***: transformer learns something about the data\n",
    "\n",
    "#### ***\"transform\":*** it uses what it learned to do the data transformation\n",
    "\n",
    "------\n",
    "\n",
    "***CountVectorizer: ***\n",
    "\n",
    "fit: learns the vocabulary\n",
    "\n",
    "transform: creates a document term matrix using the vocabulary\n",
    "\n",
    "------\n",
    "\n",
    "***SimpleImputer: ***\n",
    "\n",
    "fit: learns the value to impute\n",
    "\n",
    "transform: fills the missing value with the value to impute\n",
    "\n",
    "------\n",
    "\n",
    "***StandartScaler: ***\n",
    "\n",
    "fit: learns the mean and scale of each feature\n",
    "\n",
    "transform: standarizes the features using the mean and scale\n",
    "\n",
    "------\n",
    "\n",
    "***HashingVectorizer: ***\n",
    "\n",
    "fit: if not used, then it's known as \"stateless\" transformer\n",
    "\n",
    "transform: creates a document term matrix using a hash of the token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick2\"></a>\n",
    "# Trick 2: Seven ways to select columns using ColumnsTransformer\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# import data\n",
    "df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# select columns to transform\n",
    "X = df[[\"Fare\", \"Embarked\", \"Sex\", \"Age\"]].head()\n",
    "X[\"Age\"].iloc[0] = np.nan # create a missing values\n",
    "X\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# import the libraries\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_selector\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# instanciate the classes\n",
    "ohe = OneHotEncoder()\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# create the pipeline and select the columns by name\n",
    "\n",
    "print(\"Select the column by name\")\n",
    "\n",
    "ct = make_column_transformer(\n",
    "(ohe, [\"Embarked\", \"Sex\"])# if you have null values it will give and error. You must first fill those values before doing an ohe\n",
    ")\n",
    "\n",
    "# fit_transform the columns\n",
    "X_transformed = ct.fit_transform(X) \n",
    "X_transformed\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# create the pipeline and select the columns by position\n",
    "\n",
    "print(\"Select the column by position\")\n",
    "\n",
    "ct = make_column_transformer(\n",
    "(ohe, [1, 2])# if you have null values it will give and error. You must first fill those values before doing an ohe\n",
    ")\n",
    "\n",
    "# fit_transform the columns\n",
    "X_transformed = ct.fit_transform(X) \n",
    "X_transformed\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# create the pipeline and select the columns using slice\n",
    "\n",
    "print(\"Select the column using slice\")\n",
    "\n",
    "ct = make_column_transformer(\n",
    "(ohe, slice(1, 3))# if you have null values it will give and error. You must first fill those values before doing an ohe\n",
    ")\n",
    "\n",
    "# fit_transform the columns\n",
    "X_transformed = ct.fit_transform(X) \n",
    "X_transformed\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# create the pipeline and select the columns using a boolean mask\n",
    "\n",
    "print(\"Select the column using a boolean mask\")\n",
    "\n",
    "ct = make_column_transformer(\n",
    "(ohe, [False, True, True, False])# if you have null values it will give and error. You must first fill those values before doing an ohe\n",
    ")\n",
    "\n",
    "# fit_transform the columns\n",
    "X_transformed = ct.fit_transform(X) \n",
    "X_transformed\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# create the pipeline and select the columns using make_column_selector and regex\n",
    "\n",
    "print(\"Select the column using using make_column_selector and regex. New in pandas 0.22\")\n",
    "\n",
    "ct = make_column_transformer(\n",
    "(ohe, make_column_selector(pattern = \"E|S\"))# if you have null values it will give and error. You must first fill those values before doing an ohe\n",
    ")\n",
    "\n",
    "# fit_transform the columns\n",
    "X_transformed = ct.fit_transform(X) \n",
    "X_transformed\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# create the pipeline and select the columns using make_column_selector and dtype_include\n",
    "\n",
    "print(\"Select the column using using make_column_selector and dtype_include. New in pandas 0.22\")\n",
    "\n",
    "ct = make_column_transformer(\n",
    "(ohe, make_column_selector(dtype_include = object))# if you have null values it will give and error. You must first fill those values before doing an ohe\n",
    ")\n",
    "\n",
    "# fit_transform the columns\n",
    "X_transformed = ct.fit_transform(X) \n",
    "X_transformed\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# create the pipeline and select the columns using make_column_selector and dtype_exclude\n",
    "\n",
    "print(\"Select the column using using make_column_selector and dtype_exclude. New in pandas 0.22\")\n",
    "\n",
    "ct = make_column_transformer(\n",
    "(ohe, make_column_selector(dtype_exclude = \"number\"))# if you have null values it will give and error. You must first fill those values before doing an ohe\n",
    ")\n",
    "\n",
    "# fit_transform the columns\n",
    "X_transformed = ct.fit_transform(X) \n",
    "X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"trick1\"></a>\n",
    "# Trick 1: Using ColumnTransformer to manipulate different columns\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# import data\n",
    "df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n",
    "df.head()\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# select columns to transform\n",
    "X = df[[\"Fare\", \"Embarked\", \"Sex\", \"Age\"]].head()\n",
    "X[\"Age\"].iloc[0] = np.nan # create a missing values\n",
    "X\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# import the libraries\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# instanciate the classes\n",
    "ohe = OneHotEncoder()\n",
    "imp = SimpleImputer()\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# create the pipeline\n",
    "ct = make_column_transformer(\n",
    "(ohe, [\"Embarked\", \"Sex\"]), # if you have null values it will give and error. You must first fill those values before doing an ohe\n",
    "(imp, [\"Age\"]), \n",
    "remainder = \"passthrough\" # this means that the column Fare will appear the last one.\n",
    ")\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# fit_transform the columns\n",
    "X_transformed = ct.fit_transform(X) \n",
    "X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The End\n",
    "# Thanks a lot. If you made till the end you have learned a lot of sklearn\n",
    "[Go back to the Table of Contents](#table_of_contents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
